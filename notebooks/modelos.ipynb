{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "from nltk.tag import PerceptronTagger\n",
    "from utils import leer_csv, starts_ends_tokens, separar_spans_toxicos\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6471, 2) (2158, 2)\n"
     ]
    }
   ],
   "source": [
    "todo = leer_csv(\"../data/completo_cleaned.csv\")\n",
    "train, test = train_test_split(todo, test_size=0.25, random_state=42)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    tokens = df[\"text\"].apply(str.split)\n",
    "    vocab = set()\n",
    "    for i,row in tokens.iteritems():\n",
    "        vocab |= set(row)\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hacer_submission(model):\n",
    "    tsd_test = pd.read_csv(\"../data/tsd_test.csv\")\n",
    "\n",
    "    tsd_test.text = tsd_test.text.str.lower()\n",
    "    tokenized = tsd_test.text.apply(str.split)\n",
    "\n",
    "    submission = tokenized.apply(model.tag)\n",
    "\n",
    "    submission = submission.apply(tags2offsets)\n",
    "\n",
    "    submission.to_csv(\"../data/spans-pred.txt\", sep=\"\\t\", header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags2offsets(tagged_comment):\n",
    "    words = [word for word,tag in tagged_comment]\n",
    "    tags = [tag for word, tag in tagged_comment]\n",
    "    \n",
    "    text = \" \".join(words)\n",
    "    starts, ends = starts_ends_tokens(text)\n",
    "    offsets = []\n",
    "    for i, (word, tag) in enumerate(tagged_comment):\n",
    "        if \"-\" in tag:\n",
    "            offsets.extend(list( range(starts[i], ends[i]+1)) )\n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(system_offsets, ground_truth):\n",
    "    if ground_truth:\n",
    "        system_offsets = set(system_offsets)\n",
    "        ground_truth = set(ground_truth)\n",
    "\n",
    "        interseccion = system_offsets & ground_truth\n",
    "        precision = len(interseccion)/len(system_offsets) if system_offsets else 0\n",
    "        recall = len(interseccion)/len(ground_truth)\n",
    "        f1 = (2*precision*recall)/(precision+recall) if (precision,recall) != (0,0) else 0\n",
    "        \n",
    "    elif system_offsets:\n",
    "        f1 = 0 # no hay verdaderas, pero hay en la predicción, se define como 0\n",
    "    else:\n",
    "        f1 = 1\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LSTM](#LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con etiquetado Tóxico - No Tóxico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo oculto de Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etiquetar_secuencia(row):\n",
    "    word_starts, word_ends = starts_ends_tokens(row.text)\n",
    "    toxic_spans = separar_spans_toxicos(row)\n",
    "    toxic_starts = [span[0] for span in toxic_spans]\n",
    "    toxic_ends = [span[-1] for span in toxic_spans]\n",
    "    \n",
    "    secuencia = []\n",
    "    \n",
    "    for word_start, word_end in zip(word_starts, word_ends):\n",
    "        tag = \"-\" if word_start in toxic_starts and word_end in toxic_ends else \"+\"\n",
    "        secuencia.append(tag)\n",
    "        \n",
    "    secuencia = list(zip(row.text.split(), secuencia))\n",
    "            \n",
    "    return secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [etiquetar_secuencia(row) for row in train.itertuples()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tóxico y NO tóxico\n",
    "trainer = HiddenMarkovModelTrainer(states=[\"-\", \"+\"], symbols=vocab)\n",
    "\n",
    "hmm1 = trainer.train(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_f1(df, model):\n",
    "    tokenized_text = df.text.apply(str.split)\n",
    "    tagged_text = tokenized_text.apply(model.tag)\n",
    "\n",
    "    system_offsets = tagged_text.apply(tags2offsets)\n",
    "    scores = [f1_score(offsets, ground_truth) \n",
    "                for offsets, ground_truth in zip(system_offsets, df.spans)]\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5854434660739216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(train, hmm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28440539271538906"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(test, hmm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = PerceptronTagger(load=False)\n",
    "perceptron.train(train_sentences, nr_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328094579702002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(train, perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49023810456719485"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(test, perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacer_submission(perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con etiquetado POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etiquetar_secuencia_POS(row):\n",
    "    word_starts, word_ends = starts_ends_tokens(row.text)\n",
    "    toxic_spans = separar_spans_toxicos(row)\n",
    "    toxic_starts = [span[0] for span in toxic_spans]\n",
    "    toxic_ends = [span[-1] for span in toxic_spans]\n",
    "    \n",
    "    tokens = row.text.split()\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    secuencia = [tag for text, tag in pos_tags]\n",
    "    \n",
    "    for idx, (word_start, word_end) in enumerate(zip(word_starts, word_ends)):\n",
    "        tag = \"-\" if word_start in toxic_starts and word_end in toxic_ends else \"+\"\n",
    "        secuencia[idx] = secuencia[idx]+f\"{tag}\"\n",
    "        \n",
    "    secuencia = list(zip(row.text.split(), secuencia))\n",
    "            \n",
    "    return secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS + Tóxico y NO tóxico\n",
    "train_sentences_pos = [etiquetar_secuencia_POS(row) for row in train.itertuples()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Markov oculto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['DT+', 'NN-', 'CC-', 'JJ-', 'VBG+', 'NN+', 'CC+', 'JJ+', 'NNP+', 'PRP+', 'VBP+', 'CD+', 'NNS+', 'RB+', \n",
    "        'PRP$+', 'VBG-', 'WP+', 'VBZ+', 'VB+', 'RP+', 'IN+', 'VBN+', 'TO+', 'JJR+', 'WDT+', 'NNP-', 'WRB+', 'MD+',\n",
    "        'VB-', 'VBD+', 'VBP-', 'RBS+', 'NNS-', 'DT-', 'VBZ-', ':+', 'PDT+', 'IN-', 'EX+', '.+', 'PRP-', 'JJS+', \n",
    "        'RBR+', 'VBN-', 'NNPS+', 'CD-', 'RB-', 'WRB-', 'RP-', ',+', 'MD-', 'JJS-', '$+', 'POS+', 'NNPS-', 'SYM+', \n",
    "        'TO-', '``+', \"''-\", 'WDT-', 'VBD-', 'JJR-', 'PRP$-', 'PDT-', 'WP-', '(+', ')+', 'UH+', 'WP$+', 'FW+', \n",
    "        '#+', 'RBS-', \"''+\", 'EX-', 'RBR-', 'FW-', ':-', ',-', 'UH-', 'SYM-', '.-', 'POS-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HiddenMarkovModelTrainer(states = tags, symbols=vocab)\n",
    "\n",
    "hmm2 = trainer.train(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6603716306391332"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(train, hmm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30981471634686236"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(test, hmm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron2 = PerceptronTagger(load=False)\n",
    "perceptron2.train(train_sentences_pos, nr_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.890275118761953"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(train, perceptron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4908126887094825"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(test, perceptron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacer_submission(perceptron2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etiquetar_secuencia(row):\n",
    "    word_starts, word_ends = starts_ends_tokens(row.text)\n",
    "    toxic_spans = separar_spans_toxicos(row)\n",
    "    toxic_starts = [span[0] for span in toxic_spans]\n",
    "    toxic_ends = [span[-1] for span in toxic_spans]\n",
    "    \n",
    "    secuencia = []\n",
    "    \n",
    "    for word_start, word_end in zip(word_starts, word_ends):\n",
    "        tag = \"-\" if word_start in toxic_starts and word_end in toxic_ends else \"+\"\n",
    "        secuencia.append(tag)\n",
    "    \n",
    "    return row.text.split(), secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = train.apply(etiquetar_secuencia, axis=1).tolist()\n",
    "word_to_ix = {}\n",
    "\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix) #Asign each word with a unique index\n",
    "\n",
    "tag_to_ix = {\"-\":0, \"+\":1}  # Assign each tag with a unique index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix.get(w,0) for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                            num_layers=4, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo creado\n",
      "loss y optimizer creados\n",
      "5 tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "10 tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "15 tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "20 tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "25 tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "30 tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "35 tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "40 tensor(9.7109e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Terminó entrenamiento\n",
      "CPU times: user 23min 47s, sys: 18.5 s, total: 24min 5s\n",
      "Wall time: 24min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBEDDING_DIM = 512\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "model.to(device)\n",
    "print(\"Modelo creado\")\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "print(\"loss y optimizer creados\")\n",
    "\n",
    "for epoch in range(40):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix).to(device)\n",
    "        targets = prepare_sequence(tags, tag_to_ix).to(device)\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in).to(device)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(epoch+1, loss)\n",
    "        \n",
    "print(\"Terminó entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3705882155326365"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s = []\n",
    "ix_to_tag = [\"-\", \"+\"]\n",
    "with torch.no_grad():\n",
    "    for row in test.itertuples():\n",
    "        sentence, true_offsets = row.text.split(), row.spans\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix).to(device)\n",
    "        tags = model(sentence_in).cpu().numpy().argmax(axis=1)\n",
    "        tags = [ix_to_tag[i] for i in tags] # {0,1} -> {-,+}\n",
    "        tagged_comment = list(zip(sentence,tags))\n",
    "        offsets = tags2offsets(tagged_comment)\n",
    "        f1 = f1_score(offsets, true_offsets)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "np.mean(f1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.3830407100809408\n",
    "    * EMBEDDING_DIM = 512\n",
    "    * HIDDEN_DIM = 128\n",
    "    * loss_function = nn.NLLLoss()\n",
    "    * optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    * 40 epochs\n",
    "    * LSTM con 4 capas\n",
    "\n",
    "* 0.3705882155326365\n",
    "    * EMBEDDING_DIM = 512\n",
    "    * HIDDEN_DIM = 256\n",
    "    * loss_function = nn.NLLLoss()\n",
    "    * optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    * 40 epochs\n",
    "    * LSTM con 4 capas\n",
    "\n",
    "* 0.36030145365772975\n",
    "    * EMBEDDING_DIM = 256\n",
    "    * HIDDEN_DIM = 64\n",
    "    * loss_function = nn.NLLLoss()\n",
    "    * optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    * 40 epochs\n",
    "    * LSTM con dos capas\n",
    "\n",
    "* 0.35609794475051537\n",
    "    * EMBEDDING_DIM = 256\n",
    "    * HIDDEN_DIM = 64\n",
    "    * loss_function = nn.NLLLoss()\n",
    "    * optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    * 40 epochs\n",
    "    \n",
    "* 0.08526413345690455\n",
    "    * EMBEDDING_DIM = 256\n",
    "    * HIDDEN_DIM = 64\n",
    "    * loss_function = nn.CrossEntropyLoss()\n",
    "    * optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    * 40 epochs\n",
    "    \n",
    "* 0.08526413345690455\n",
    "    * EMBEDDING_DIM = 512\n",
    "    * HIDDEN_DIM = 128\n",
    "    * loss_function = nn.NLLLoss()\n",
    "    * optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    * 40 epochs\n",
    "    * LSTM con 8 capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Inicio](#Datos-y-funciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
