{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dante/miniconda3/envs/pln/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "from nltk.tag import PerceptronTagger\n",
    "from utils import leer_csv, starts_ends_tokens, separar_spans_toxicos\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6471, 2) (2158, 2)\n"
     ]
    }
   ],
   "source": [
    "todo = leer_csv(\"../data/completo_cleaned.csv\")\n",
    "train, test = train_test_split(todo, test_size=0.25, random_state=42)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df):\n",
    "    tokens = df[\"text\"].apply(str.split)\n",
    "    vocab = set()\n",
    "    for i,row in tokens.iteritems():\n",
    "        vocab |= set(row)\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hacer_submission(model):\n",
    "    tsd_test = pd.read_csv(\"../data/tsd_test.csv\")\n",
    "\n",
    "    tsd_test.text = tsd_test.text.str.lower()\n",
    "    tokenized = tsd_test.text.apply(str.split)\n",
    "\n",
    "    submission = tokenized.apply(model.tag)\n",
    "\n",
    "    submission = submission.apply(tags2offsets)\n",
    "\n",
    "    submission.to_csv(\"../data/spans-pred.txt\", sep=\"\\t\", header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con etiquetado Tóxico - No Tóxico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo oculto de Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etiquetar_secuencia(row):\n",
    "    word_starts, word_ends = starts_ends_tokens(row.text)\n",
    "    toxic_spans = separar_spans_toxicos(row)\n",
    "    toxic_starts = [span[0] for span in toxic_spans]\n",
    "    toxic_ends = [span[-1] for span in toxic_spans]\n",
    "    \n",
    "    secuencia = []\n",
    "    \n",
    "    for word_start, word_end in zip(word_starts, word_ends):\n",
    "        tag = \"-\" if word_start in toxic_starts and word_end in toxic_ends else \"+\"\n",
    "        secuencia.append(tag)\n",
    "        \n",
    "    secuencia = list(zip(row.text.split(), secuencia))\n",
    "            \n",
    "    return secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [etiquetar_secuencia(row) for row in train.itertuples()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tóxico y NO tóxico\n",
    "trainer = HiddenMarkovModelTrainer(states=[\"-\", \"+\"], symbols=vocab)\n",
    "\n",
    "hmm1 = trainer.train(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags2offsets(tagged_comment):\n",
    "    words = [word for word,tag in tagged_comment]\n",
    "    tags = [tag for word, tag in tagged_comment]\n",
    "    \n",
    "    text = \" \".join(words)\n",
    "    starts, ends = starts_ends_tokens(text)\n",
    "    offsets = []\n",
    "    for i, (word, tag) in enumerate(tagged_comment):\n",
    "        if \"-\" in tag:\n",
    "            offsets.extend(list( range(starts[i], ends[i]+1)) )\n",
    "    return offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(system_offsets, ground_truth):\n",
    "    if ground_truth:\n",
    "        system_offsets = set(system_offsets)\n",
    "        ground_truth = set(ground_truth)\n",
    "\n",
    "        interseccion = system_offsets & ground_truth\n",
    "        precision = len(interseccion)/len(system_offsets) if system_offsets else 0\n",
    "        recall = len(interseccion)/len(ground_truth)\n",
    "        f1 = (2*precision*recall)/(precision+recall) if (precision,recall) != (0,0) else 0\n",
    "        \n",
    "    elif system_offsets:\n",
    "        f1 = 0 # no hay verdaderas, pero hay en la predicción, se define como 0\n",
    "    else:\n",
    "        f1 = 1\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_f1(df, model):\n",
    "    tokenized_text = df.text.apply(str.split)\n",
    "    tagged_text = tokenized_text.apply(model.tag)\n",
    "\n",
    "    system_offsets = tagged_text.apply(tags2offsets)\n",
    "    scores = [f1_score(offsets, ground_truth) \n",
    "                for offsets, ground_truth in zip(system_offsets, df.spans)]\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5854434660739216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(train, hmm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28440539271538906"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(test, hmm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = PerceptronTagger(load=False)\n",
    "perceptron.train(train_sentences, nr_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328094579702002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(train, perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49023810456719485"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(test, perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacer_submission(perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con etiquetado POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etiquetar_secuencia_POS(row):\n",
    "    word_starts, word_ends = starts_ends_tokens(row.text)\n",
    "    toxic_spans = separar_spans_toxicos(row)\n",
    "    toxic_starts = [span[0] for span in toxic_spans]\n",
    "    toxic_ends = [span[-1] for span in toxic_spans]\n",
    "    \n",
    "    tokens = row.text.split()\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    secuencia = [tag for text, tag in pos_tags]\n",
    "    \n",
    "    for idx, (word_start, word_end) in enumerate(zip(word_starts, word_ends)):\n",
    "        tag = \"-\" if word_start in toxic_starts and word_end in toxic_ends else \"+\"\n",
    "        secuencia[idx] = secuencia[idx]+f\"{tag}\"\n",
    "        \n",
    "    secuencia = list(zip(row.text.split(), secuencia))\n",
    "            \n",
    "    return secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS + Tóxico y NO tóxico\n",
    "train_sentences_pos = [etiquetar_secuencia_POS(row) for row in train.itertuples()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Markov oculto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['DT+', 'NN-', 'CC-', 'JJ-', 'VBG+', 'NN+', 'CC+', 'JJ+', 'NNP+', 'PRP+', 'VBP+', 'CD+', 'NNS+', 'RB+', \n",
    "        'PRP$+', 'VBG-', 'WP+', 'VBZ+', 'VB+', 'RP+', 'IN+', 'VBN+', 'TO+', 'JJR+', 'WDT+', 'NNP-', 'WRB+', 'MD+',\n",
    "        'VB-', 'VBD+', 'VBP-', 'RBS+', 'NNS-', 'DT-', 'VBZ-', ':+', 'PDT+', 'IN-', 'EX+', '.+', 'PRP-', 'JJS+', \n",
    "        'RBR+', 'VBN-', 'NNPS+', 'CD-', 'RB-', 'WRB-', 'RP-', ',+', 'MD-', 'JJS-', '$+', 'POS+', 'NNPS-', 'SYM+', \n",
    "        'TO-', '``+', \"''-\", 'WDT-', 'VBD-', 'JJR-', 'PRP$-', 'PDT-', 'WP-', '(+', ')+', 'UH+', 'WP$+', 'FW+', \n",
    "        '#+', 'RBS-', \"''+\", 'EX-', 'RBR-', 'FW-', ':-', ',-', 'UH-', 'SYM-', '.-', 'POS-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HiddenMarkovModelTrainer(states = tags, symbols=vocab)\n",
    "\n",
    "hmm2 = trainer.train(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6603716306391332"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(train, hmm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30981471634686236"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_f1(test, hmm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron2 = PerceptronTagger(load=False)\n",
    "perceptron2.train(train_sentences_pos, nr_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtener_f1(train, perceptron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtener_f1(test, perceptron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
